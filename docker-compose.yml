version: '3'

services:
  vllm:
    container_name: vllm
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - torch_cuda_arch_list=8.9+PTX # NVIDIA L4
        - max_jobs=2 # Tune down if out of memory
        - nvcc_threads=8
      target: vllm-openai
    command: >
      --model Intel/neural-chat-7b-v3-1
      --chat-template /examples/template_neuralchat.jinja
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface # model cache
      - ./examples:/examples # chat template
    ports:
      - 8000:8000
    healthcheck:
      test: ["CMD", "python3", "-c", "import http.client; conn = http.client.HTTPConnection('localhost', 8000); conn.request('GET', '/health'); response = conn.getresponse(); exit(0) if response.status == 200 else exit(1)"]
      interval: 1m
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  webui:
    container_name: webui
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.10
        RUN pip install --no-cache-dir openai gradio
        COPY client.py /client.py
        ENV VLLM_ENDPOINT=http://vllm:8000/v1
        EXPOSE 7860
        CMD ["python", "client.py"]
    ports:
      - 7860:7860
    restart: unless-stopped
    depends_on:
      vllm:
        condition: service_healthy
